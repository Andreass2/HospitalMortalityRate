{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "###############################\n",
    "# Non-time series preprocessing\n",
    "###############################\n",
    "\n",
    "g_map = { 'F': 1, 'M': 2, 'OTHER': 3, '': 0 }\n",
    "def transform_gender(gender_series):\n",
    "    global g_map\n",
    "    return { 'Gender': gender_series.fillna('').apply(lambda s: g_map[s] if s in g_map else g_map['OTHER']) }\n",
    "\n",
    "e_map = { 'ASIAN': 1,\n",
    "          'BLACK': 2,\n",
    "          'HISPANIC': 3,\n",
    "          'WHITE': 4,\n",
    "          'OTHER': 5, # map everything else to 5 (OTHER)\n",
    "          'UNABLE TO OBTAIN': 0,\n",
    "          'PATIENT DECLINED TO ANSWER': 0,\n",
    "          'UNKNOWN': 0,\n",
    "          '': 0 }\n",
    "def transform_ethnicity(ethnicity_series):\n",
    "    global e_map\n",
    "    \n",
    "    def aggregate_ethnicity(ethnicity_str):\n",
    "        return ethnicity_str.replace(' OR ', '/').split(' - ')[0].split('/')[0]\n",
    "\n",
    "    ethnicity_series = ethnicity_series.apply(aggregate_ethnicity)\n",
    "    return { 'Ethnicity': ethnicity_series.fillna('').apply(lambda s: e_map[s] if s in e_map else e_map['OTHER']) }\n",
    "\n",
    "def assemble_episodic_data(stays, diagnoses):\n",
    "    data = { 'Icustay': stays.ICUSTAY_ID, 'Age': stays.AGE, 'Length of Stay': stays.LOS,\n",
    "                    'Mortality': stays.MORTALITY }\n",
    "    data.update(transform_gender(stays.GENDER))\n",
    "    data.update(transform_ethnicity(stays.ETHNICITY))\n",
    "    data['Height'] = np.nan\n",
    "    data['Weight'] = np.nan\n",
    "    data = pd.DataFrame(data).set_index('Icustay')\n",
    "    data = data[['Ethnicity', 'Gender', 'Age', 'Height', 'Weight', 'Length of Stay', 'Mortality']]\n",
    "    return data.merge(extract_diagnosis_labels(diagnoses), left_index=True, right_index=True)\n",
    "\n",
    "diagnosis_labels = [ '4019', '4280', '41401', '42731', '25000', '5849', '2724', '51881', '53081', '5990', '2720', '2859', '2449', '486', '2762', '2851', '496', 'V5861', '99592', '311', '0389', '5859', '5070', '40390', '3051', '412', 'V4581', '2761', '41071', '2875', '4240', 'V1582', 'V4582', 'V5867', '4241', '40391', '78552', '5119', '42789', '32723', '49390', '9971', '2767', '2760', '2749', '4168', '5180', '45829', '4589', '73300', '5845', '78039', '5856', '4271', '4254', '4111', 'V1251', '30000', '3572', '60000', '27800', '41400', '2768', '4439', '27651', 'V4501', '27652', '99811', '431', '28521', '2930', '7907', 'E8798', '5789', '79902', 'V4986', 'V103', '42832', 'E8788', '00845', '5715', '99591', '07054', '42833', '4275', '49121', 'V1046', '2948', '70703', '2809', '5712', '27801', '42732', '99812', '4139', '3004', '2639', '42822', '25060', 'V1254', '42823', '28529', 'E8782', '30500', '78791', '78551', 'E8889', '78820', '34590', '2800', '99859', 'V667', 'E8497', '79092', '5723', '3485', '5601', '25040', '570', '71590', '2869', '2763', '5770', 'V5865', '99662', '28860', '36201', '56210' ]\n",
    "def extract_diagnosis_labels(diagnoses):\n",
    "    global diagnosis_labels\n",
    "    diagnoses['VALUE'] = 1\n",
    "    labels = diagnoses[['ICUSTAY_ID', 'ICD9_CODE', 'VALUE']].drop_duplicates().pivot(index='ICUSTAY_ID', columns='ICD9_CODE', values='VALUE').fillna(0).astype(int)    \n",
    "    for l in diagnosis_labels:\n",
    "        if l not in labels:\n",
    "            labels[l] = 0\n",
    "    labels = labels[diagnosis_labels]\n",
    "    return labels.rename(dict(zip(diagnosis_labels, [ 'Diagnosis ' + d for d in diagnosis_labels])), axis=1)\n",
    "\n",
    "def add_hcup_ccs_2015_groups(diagnoses, definitions):\n",
    "    def_map = {}\n",
    "    for dx in definitions:\n",
    "        for code in definitions[dx]['codes']:\n",
    "            def_map[code] = (dx, definitions[dx]['use_in_benchmark'])\n",
    "    diagnoses['HCUP_CCS_2015'] = diagnoses.ICD9_CODE.apply(lambda c: def_map[c][0] if c in def_map else None)\n",
    "    diagnoses['USE_IN_BENCHMARK'] = diagnoses.ICD9_CODE.apply(lambda c: int(def_map[c][1]) if c in def_map else None)\n",
    "    return diagnoses\n",
    "\n",
    "def make_phenotype_label_matrix(phenotypes, stays=None):\n",
    "    phenotypes = phenotypes[['ICUSTAY_ID', 'HCUP_CCS_2015']].loc[phenotypes.USE_IN_BENCHMARK > 0].drop_duplicates()\n",
    "    phenotypes['VALUE'] = 1\n",
    "    phenotypes = phenotypes.pivot(index='ICUSTAY_ID', columns='HCUP_CCS_2015', values='VALUE')\n",
    "    if stays is not None:\n",
    "        phenotypes = phenotypes.loc[stays.ICUSTAY_ID.sort_values()]\n",
    "    return phenotypes.fillna(0).astype(int).sort_index(axis=0).sort_index(axis=1)\n",
    "\n",
    "\n",
    "###################################\n",
    "# Time series preprocessing\n",
    "###################################\n",
    "\n",
    "def read_itemid_to_variable_map(fn, variable_column='LEVEL2'):\n",
    "    var_map = pd.read_csv(fn, index_col=None).fillna('').astype(str)\n",
    "    #var_map[variable_column] = var_map[variable_column].apply(lambda s: s.lower())\n",
    "    var_map.COUNT = var_map.COUNT.astype(int)\n",
    "    var_map = var_map.loc[(var_map[variable_column] != '') & (var_map.COUNT>0)]\n",
    "    var_map = var_map.loc[(var_map.STATUS == 'ready')]\n",
    "    var_map.ITEMID = var_map.ITEMID.astype(int)\n",
    "    var_map = var_map[[variable_column, 'ITEMID', 'MIMIC LABEL']].set_index('ITEMID')\n",
    "    return var_map.rename({variable_column: 'VARIABLE', 'MIMIC LABEL': 'MIMIC_LABEL'}, axis=1)\n",
    "\n",
    "def map_itemids_to_variables(events, var_map):\n",
    "    return events.merge(var_map, left_on='ITEMID', right_index=True)\n",
    "\n",
    "def read_variable_ranges(fn, variable_column='LEVEL2'):\n",
    "    columns = [ variable_column, 'OUTLIER LOW', 'VALID LOW', 'IMPUTE', 'VALID HIGH', 'OUTLIER HIGH' ]\n",
    "    to_rename = dict(zip(columns, [ c.replace(' ', '_') for c in columns ]))\n",
    "    to_rename[variable_column] = 'VARIABLE'\n",
    "    var_ranges = pd.read_csv(fn, index_col=None)\n",
    "    #var_ranges = var_ranges[variable_column].apply(lambda s: s.lower())\n",
    "    var_ranges = var_ranges[columns]\n",
    "    var_ranges.rename(to_rename, axis=1, inplace=True)\n",
    "    var_ranges = var_ranges.drop_duplicates(subset='VARIABLE', keep='first')\n",
    "    var_ranges.set_index('VARIABLE', inplace=True)\n",
    "    return var_ranges.loc[var_ranges.notnull().all(axis=1)]\n",
    "\n",
    "def remove_outliers_for_variable(events, variable, ranges):\n",
    "    if variable not in ranges.index:\n",
    "        return events\n",
    "    idx = (events.VARIABLE == variable)\n",
    "    V = events.VALUE[idx]\n",
    "    V.loc[V < ranges.OUTLIER_LOW[variable]]  = np.nan\n",
    "    V.loc[V > ranges.OUTLIER_HIGH[variable]] = np.nan\n",
    "    V.loc[V < ranges.VALID_LOW[variable]]    = ranges.VALID_LOW[variable]\n",
    "    V.loc[V > ranges.VALID_HIGH[variable]]   = ranges.VALID_HIGH[variable]\n",
    "    events.loc[idx,'VALUE'] = V\n",
    "    return events\n",
    "\n",
    "# SBP: some are strings of type SBP/DBP\n",
    "def clean_sbp(df):\n",
    "    v = df.VALUE.astype(str)\n",
    "    idx = v.apply(lambda s: '/' in s)\n",
    "    v.loc[idx] = v[idx].apply(lambda s: re.match('^(\\d+)/(\\d+)$', s).group(1))\n",
    "    return v.astype(float)\n",
    "\n",
    "def clean_dbp(df):\n",
    "    v = df.VALUE.astype(str)\n",
    "    idx = v.apply(lambda s: '/' in s)\n",
    "    v.loc[idx] = v[idx].apply(lambda s: re.match('^(\\d+)/(\\d+)$', s).group(2))\n",
    "    return v.astype(float)\n",
    "\n",
    "# CRR: strings with brisk, <3 normal, delayed, or >3 abnormal\n",
    "def clean_crr(df):\n",
    "    v = pd.Series(np.zeros(df.shape[0]), index=df.index)\n",
    "    v[:] = np.nan\n",
    "    \n",
    "    # when df.VALUE is empty, dtype can be float and comparision with string\n",
    "    # raises an exception, to fix this we change dtype to str\n",
    "    df.VALUE = df.VALUE.astype(str)\n",
    "    \n",
    "    v.loc[(df.VALUE == 'Normal <3 secs') | (df.VALUE == 'Brisk')] = 0\n",
    "    v.loc[(df.VALUE == 'Abnormal >3 secs') | (df.VALUE == 'Delayed')] = 1\n",
    "    return v\n",
    "\n",
    "# FIO2: many 0s, some 0<x<0.2 or 1<x<20\n",
    "def clean_fio2(df):\n",
    "    v = df.VALUE.astype(float)\n",
    "    #idx = df.VALUEUOM.fillna('').apply(lambda s: 'torr' not in s.lower()) & (v>1.0)\n",
    "    idx = df.VALUEUOM.fillna('').apply(lambda s: 'torr' not in s.lower()) & (df.VALUE>1.0)\n",
    "    v.loc[idx] = v[idx] / 100.\n",
    "    return v\n",
    "\n",
    "# GLUCOSE, PH: sometimes have ERROR as value\n",
    "def clean_lab(df):\n",
    "    v = df.VALUE\n",
    "    idx = v.apply(lambda s: type(s) is str and not re.match('^(\\d+(\\.\\d*)?|\\.\\d+)$', s))\n",
    "    v.loc[idx] = np.nan\n",
    "    return v.astype(float)\n",
    "\n",
    "# O2SAT: small number of 0<x<=1 that should be mapped to 0-100 scale\n",
    "def clean_o2sat(df):\n",
    "    # change \"ERROR\" to NaN\n",
    "    v = df.VALUE\n",
    "    idx = v.apply(lambda s: type(s) is str and not re.match('^(\\d+(\\.\\d*)?|\\.\\d+)$', s))\n",
    "    v.loc[idx] = np.nan\n",
    "    \n",
    "    v = v.astype(float)\n",
    "    idx = (v<=1)\n",
    "    v.loc[idx] = v[idx] * 100.\n",
    "    return v\n",
    "\n",
    "# Temperature: map Farenheit to Celsius, some ambiguous 50<x<80\n",
    "def clean_temperature(df):\n",
    "    v = df.VALUE.astype(float)\n",
    "    idx = df.VALUEUOM.fillna('').apply(lambda s: 'F' in s.lower()) | df.MIMIC_LABEL.apply(lambda s: 'F' in s.lower()) | (v >= 79)\n",
    "    v.loc[idx] = (v[idx] - 32) * 5. / 9\n",
    "    return v\n",
    "\n",
    "# Weight: some really light/heavy adults: <50 lb, >450 lb, ambiguous oz/lb\n",
    "# Children are tough for height, weight\n",
    "def clean_weight(df):\n",
    "    v = df.VALUE.astype(float)\n",
    "    # ounces\n",
    "    idx = df.VALUEUOM.fillna('').apply(lambda s: 'oz' in s.lower()) | df.MIMIC_LABEL.apply(lambda s: 'oz' in s.lower())\n",
    "    v.loc[idx] = v[idx] / 16.\n",
    "    # pounds\n",
    "    idx = idx | df.VALUEUOM.fillna('').apply(lambda s: 'lb' in s.lower()) | df.MIMIC_LABEL.apply(lambda s: 'lb' in s.lower())\n",
    "    v.loc[idx] = v[idx] * 0.453592\n",
    "    return v\n",
    "\n",
    "# Height: some really short/tall adults: <2 ft, >7 ft)\n",
    "# Children are tough for height, weight\n",
    "def clean_height(df):\n",
    "    v = df.VALUE.astype(float)\n",
    "    idx = df.VALUEUOM.fillna('').apply(lambda s: 'in' in s.lower()) | df.MIMIC_LABEL.apply(lambda s: 'in' in s.lower())\n",
    "    v.loc[idx] = np.round(v[idx] * 2.54)\n",
    "    return v\n",
    "\n",
    "# ETCO2: haven't found yet\n",
    "# Urine output: ambiguous units (raw ccs, ccs/kg/hr, 24-hr, etc.)\n",
    "# Tidal volume: tried to substitute for ETCO2 but units are ambiguous\n",
    "# Glascow coma scale eye opening\n",
    "# Glascow coma scale motor response\n",
    "# Glascow coma scale total\n",
    "# Glascow coma scale verbal response\n",
    "# Heart Rate\n",
    "# Respiratory rate\n",
    "# Mean blood pressure\n",
    "clean_fns = {\n",
    "    'Capillary refill rate': clean_crr,\n",
    "    'Diastolic blood pressure': clean_dbp,\n",
    "    'Systolic blood pressure': clean_sbp,\n",
    "    'Fraction inspired oxygen': clean_fio2,\n",
    "    'Oxygen saturation': clean_o2sat,\n",
    "    'Glucose': clean_lab,\n",
    "    'pH': clean_lab,\n",
    "    'Temperature': clean_temperature,\n",
    "    'Weight': clean_weight,\n",
    "    'Height': clean_height\n",
    "}\n",
    "def clean_events(events):\n",
    "    global cleaning_fns\n",
    "    for var_name, clean_fn in clean_fns.items():\n",
    "        idx = (events.VARIABLE == var_name)\n",
    "        try:\n",
    "            events.loc[idx,'VALUE'] = clean_fn(events.loc[idx])\n",
    "        except Exception as e:\n",
    "            print(\"Exception in clean_events:\", clean_fn.__name__, e)\n",
    "            print(\"number of rows:\", np.sum(idx))\n",
    "            print(\"values:\", events.loc[idx])\n",
    "            exit()\n",
    "    return events.loc[events.VALUE.notnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
